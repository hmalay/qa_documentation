{
    "docs": [
        {
            "location": "/", 
            "text": "Scholastic Test Plans", 
            "title": "Home"
        }, 
        {
            "location": "/#scholastic-test-plans", 
            "text": "", 
            "title": "Scholastic Test Plans"
        }, 
        {
            "location": "/methodology/", 
            "text": "QA Test Planning Methodology\n\n\nPlanning\n\n\nTo begin the planning stage, QA's have to meet with developers and/or product owners and/or clients to discuss the documentation and project scope prior to implementation to begin the planning stage. Documentation comes in various forms, from paper to electronic, but no matter what kind, these product/project documents have to be analyzed. The time reading it should also be time dedicated to looking for any discrepancies and issues, meanwhile also building and spec'ing test dependancies, frameworks and cases. The product document is examined thoroughly for complexity, UI features, functionality, issues and most importantly \nrequirements\n; some aspects may need to be changed to work. Analysis as a QA is feedback for the developers.\n\n\nImplementation\n\n\nFeedback to developers signals the beginning of the implementation stage. The product is placed into the warm and loving hands of development. During this stage QA's begin to build test cases for the upcoming product. These cases are built with the concepts in Test Plans parenting Test Cases/Scenarios. Each test is designed to examine the product as per the Scope Document(s). After further documentation is processed, the QA's begin building test cases again to examine the code that has been provided. The testing goes on until all elements of the product have been tested thoroughly.\n\n\nSoftware testing is the process of validating and verifying that a software program/application/product:\n\n\n\n\nMeets the requirements that guided its design and development;\n\n\nWorks as expected; and\n\n\nCan be implemented with the same characteristics in other environments\n\n\n\n\nDelivery \n Maintenance\n\n\nThis is the bulk of the QA work. Regression tests are executed during this phase, Manual or Automation. After more tests have been built to meet the needs of client based functionalities, a UAT has to be drafted and handed out to the client(s), internal and/or external.\n\n\nThis UAT (User Acceptance Test) tests all up-to-date functions of the software. During this phase it is also key to build test plans, tests that check the code in a regimented fashion. These tests also have to be executed, heeding to the Test Schedule. All test results have to be delivered, in a Test Execution Report. \nEverything has to be reproducible.\n\n\nTest Types\n\n\nDuring the planning stage, QA Testers build test cases. Test cases and test plans require a full assessment of the documentation.\n\n\nThere are many kinds of testing and they include but are not limited to:\n\n\n\n\n\n\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBlack-Box\n\n\nInternal system design is not considered in this type of testing. Tests are based on requirements and functionality.\n\n\n\n\n\n\nWhite -Box\n\n\nThis testing is based on knowledge of the internal logic of an application's code. Also known as Glass box Testing. Internal software and code working should be known for this type of testing. Tests are based on coverage of code statements, branches, paths, conditions.\n\n\n\n\n\n\nUnit\n\n\nTesting of individual software components. Typically done by the programmer and not by testers, as it requires detailed knowledge of the internal program design and code. May require developing test driver modules or test harnesses.\n\n\n\n\n\n\nIncremental Integration\n\n\nBottom up approach for testing i.e. continuous testing of an application as new functionality is added; Application functionality and modules should be independent enough to test separately. Done by programmers or by testers.\n\n\n\n\n\n\nIntegration\n\n\nTesting of integrated modules to verify functionality after integration. Modules are typically code modules, individual applications, client and server applications on a network, etc. This type of testing is especially relevant to client/server and distributed systems.\n\n\n\n\n\n\nFunctional\n\n\nThis type of testing ignores the internal parts and focus on the output is as per requirement or not. Black-box type testing geared to functional requirements of an application.\n\n\n\n\n\n\nSystem\n\n\nEntire system is tested as per the requirements. Black-box type testing that is based on overall requirements/specifications. Covers all combined parts of a system.\n\n\n\n\n\n\nEnd-to-End\n\n\nSimilar to system testing, involves testing of a complete application environment in a situation that mimics real-world use, such as interacting with a database, using network communications, or interacting with other hardware, applications, or systems if appropriate.\n\n\n\n\n\n\nSanity\n\n\nTesting to determine if a new software version is performing well enough to accept it for a major testing effort. If application is crashing for initial use then system is not stable enough for further testing and build or application is assigned to fix.\n\n\n\n\n\n\nRegression\n\n\nTesting the application as a whole for the modification in any module or functionality. Difficult to cover all the system in regression testing so typically automation tools are used for these testing types.\n\n\n\n\n\n\nAcceptance, UAT\n\n\nNormally this type of testing is done to verify if system meets the customer specified requirements. Users or customers use this testing to determine whether to accept application.\n\n\n\n\n\n\nLoad\n\n\nIt's a performance testing to check system behavior under load. Testing an application under heavy loads, such as testing of a web site under a range of loads to determine at what point the system response time degrades or fails.\n\n\n\n\n\n\nStress\n\n\nSystem is stressed beyond its specifications to check how and when it fails. Performed under heavy load like putting large number beyond storage capacity, complex database queries, continuous input to system or database load.\n\n\n\n\n\n\nPerformance\n\n\nTerm often used interchangeably with stress and load testing. To check whether system meets performance requirements. Used different performance and load tools to do this.\n\n\n\n\n\n\nUsability\n\n\nUser-friendliness check. Application flow is tested, Can new user understand the application easily, Proper help documented whenever user stuck at any point. Basically system navigation is checked in this testing.\n\n\n\n\n\n\nInstallation\n\n\nTested for full, partial, or upgrade install/uninstall processes on different operating systems under different hardware, software environment.\n\n\n\n\n\n\nRecovery\n\n\nTesting how well a system recovers from crashes, hardware failures, or other catastrophic problems.\n\n\n\n\n\n\nSecurity\n\n\nCan system be penetrated by any hacking way. Testing how well the system protects against unauthorized internal or external access. Checked if system, database is safe from external attacks.\n\n\n\n\n\n\nCompatibility\n\n\nTesting how well software performs in a particular hardware/software/operating system/network environment and different combinations of above.\n\n\n\n\n\n\nComparison\n\n\nComparison of product strengths and weaknesses with previous versions or other similar products.\n\n\n\n\n\n\nAlpha\n\n\nIn house virtual user environment can be created for this type of testing. Testing is done at the end of development. Still minor design changes may be made as a result of such testing.\n\n\n\n\n\n\nBeta\n\n\nTesting typically done by end-users or others. Final testing before releasing application for commercial purpose.\n\n\n\n\n\n\n\n\nTest Plans\n\n\nEarly in the deployment planning phase, the testing team creates a test plan. The test plan defines the objectives and scope of the testing effort, and identifies the methodology that your team will use to conduct tests. It also identifies the hardware, software, and tools required for testing and the features and functions that will be tested. A well-rounded test plan notes any risk factors that jeopardize testing and includes a testing schedule.\n\n\nIf your testing team is divided into technology sub-teams, each sub-team should develop a test plan for that team's specific technology area. For example, the networking team would write a test plan for testing networking features. All members of each sub-team should review and approve its team's test plan before it is integrated into the general test plan.\n\n\nDefining Test Scope\n\n\nIn the scope and objectives section of the test plan, the testing team describes specifically what you want your testing to accomplish.\n\n\nAlso, you need to define the scope of your testing by identifying what you will test and what you will not. For example, you might limit your testing of client computer hardware to the minimum supported configurations or to the standard configurations.\n\n\nIdentify Requirements\n\n\n\n\n\n\n\n\nRequirement\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHardware\n\n\nList the hardware that you need to conduct tests. Include components such as video cards, modems, routers and external drives.\n\n\n\n\n\n\nSoftware\n\n\nList the software that you need to test the compatibility of your applications within defined environments.\n\n\n\n\n\n\nEnvironments\n\n\nInclude databases or environments that you need to prepare for testing applications. Also include a description of the resources that you need to populate the databases, such as personal and business data.\n\n\n\n\n\n\nPersonnel\n\n\nIdentify the number of testers you need and the skill level required. Include consultants and other support personnel, as necessary.\n\n\n\n\n\n\nTraining\n\n\nFor example, any additional learning or training that your testers need to complete prior to testing their assigned applications or technologies.\n\n\n\n\n\n\nTools\n\n\nInclude all tools or scripts that you need to automate testing and to track results.\n\n\n\n\n\n\n\n\nList Features\n\n\nList all the features or aspects of features that need to be tested. This part of the test plan describes what to test, not how to test. For example:\n\n\n\n\nContact Form\n\n\nCheckout/Shopping Cart\n\n\nRegistration \n Log In\n\n\nAccount Management\n\n\nRMA Return Management\n\n\n\n\nDefine Acceptance Criteria\n\n\nAcceptance criteria is to be determined by the User Stories that make up the MVP. The user stories will follow but the test coverage has to ensure:\n\n\n\n\nThe functionality of each feature and service that implemented\n\n\nInteroperability with existing components and systems in the production environment, both during the phase-in period and after the environment has been rolled out\n\n\nHardware and driver compatibility for every type of client computer\n\n\nApplication compatibility for every application\n\n\nApplication compatibility for every server application\n\n\nOptimization of configurations, such as those for standardized desktops on client computers\n\n\nBaselines (a range of measurements derived from performance monitoring that represents acceptable performance under typical conditions) for performance monitoring\n\n\nBaselines and stress tests for capacity planning\n\n\nProcedures for deployment and post-deployment administration, such as procedures for upgrading a client computer and for backing out of a faulty rollout process.\n\n\nUses the required tools and utilities\n\n\nDescribes the risk factors that could prevent the successful completion of all required tests.\n\n\n\n\nCreate a Test Schedule\n\n\nDraft a preliminary schedule that includes each test listed in the test plan. The schedule can help you coordinate test lab use among sub-teams. Assign a team member, ideally the test lab manager, if your team has one, to maintain and update the lab schedule. Having an up-to-date schedule is critical when unscheduled lab requests are submitted.\n\n\nTesting occurs through all phases of development. The dates below denote stress, forced-error, UAT, End to End and System testing prior to client review. Testing will continue to occur during development and integration. Automation and unit tests to be completed in parallel to manual testing, if feasible and necessary. For example:\n\n\n\n\n\n\n\n\nTask Name\n\n\nDuration (days)\n\n\nStart\n\n\nFinish\n\n\n\n\n\n\n\n\n\n\nApplication Name\n\n\n36\n\n\nMM/DD/YYYY\n\n\nMM/DD/YYYY\n\n\n\n\n\n\nDevelopment\n\n\n22\n\n\nMM/DD/YYYY\n\n\nMM/DD/YYYY\n\n\n\n\n\n\nQA Review #1 - Theta Testing\n\n\n2\n\n\nMM/DD/YYYY\n\n\nMM/DD/YYYY\n\n\n\n\n\n\nDevelopment (Revisions)\n\n\n2\n\n\nMM/DD/YYYY\n\n\nMM/DD/YYYY\n\n\n\n\n\n\nDeployment to QA Environment\n\n\n2\n\n\nMM/DD/YYYY\n\n\nMM/DD/YYYY\n\n\n\n\n\n\nQA Review #2 - Beta Testing\n\n\n1\n\n\nMM/DD/YYYY\n\n\nMM/DD/YYYY\n\n\n\n\n\n\nDeployment to Stage or Alpha Environment\n\n\n2\n\n\nMM/DD/YYYY\n\n\nMM/DD/YYYY\n\n\n\n\n\n\nFinal QA - Alpha Testing\n\n\n5\n\n\nMM/DD/YYYY\n\n\nMM/DD/YYYY\n\n\n\n\n\n\n\n\nDefine Epic User Stories\n\n\nList all the features or aspects of features that need to be tested. This part of the test plan describes what to test, not how to test. The terminology and format are not limited to any type. However, user stories are business-readable and proficient.\n\n\nUser Stories \n*\n\n\nTraditional user-story template\n\n\nAs a \nrole\n,\nI want \ngoal/desire\n,\nSo that \nbenefit\n\n\n\n\nThe \"so that\" clause as optional.\n\n\nHunting the value\n\n\nIn order to \nreceive benefit\n,\nAs a \nrole\n,\nI want \ngoal/desire\n\n\n\n\nFive W's specifies:\n\n\nAs \nwho\n \nwhen\n \nwhere\n,\nI \nwhat\n,\nBecause \nwhy\n\n\n\n\nDefine Scenarios\n\n\nA test case or scenario is a detailed procedure that fully tests a feature or an aspect of a feature. Whereas the test plan describes what is to be tested, a test case describes how to perform a particular test. You need to develop a test case for each test listed in the test plan or test specification.\n\n\nTest cases should be written by someone who understands the function or technology being tested and should go through a peer review.\n\n\nTest cases include information such as the following:\n\n\n\n\nPurpose of the test\n\n\nSpecial hardware requirements, such as a modem\n\n\nSpecial software requirements, such as a tool\n\n\nSpecific setup or configuration requirements\n\n\nDescription of how to perform the test\n\n\nExpected results or success criteria for the test\n\n\nDesigning test cases can be a time-consuming phase in your testing schedule. Although you might be tempted to take shortcuts, the time you spend will pay off in the long run.\n\n\n\n\nYou can conduct tests faster when they are carefully planned. Otherwise, testers spend time debugging and rerunning tests.\n\n\nOrganizations take a variety of approaches to documenting test cases; these range from developing detailed, recipe-like steps to writing general descriptions. In detailed test cases, the steps describe exactly how to perform the test. In descriptive test cases, the tester decides at the time of the test how to perform the test and what data to use.\n\n\nSome advantages of detailed test cases are that they are reproducible and they are easier to automate. This approach is particularly important if you plan to compare the results of tests over time, such as when you are optimizing configurations. A disadvantage of detailed test cases is that they are more time-consuming to develop and maintain. On the other hand, test cases that are open to interpretation are not repeatable and can lead to debugging time that pertains more to the test itself than to what is being tested.\n\n\nIt is recommended that you find a compromise between the two extremes, one that tends toward more detail. Balance thoroughness with practicality to reach your goal of test integrity and manageability.\n\n\nAdvantages of Gherkin-based Tests\n\n\nScenarios can be written in a multitude of ways. However, when it comes to standardization, its hard to hit the nail on the head as uniquely as Gherkin does. Even if AUT uses a JavaScript or fully-Manual testing solution, business-readable scenarios go along way for\n\n\nGherkin scenarios\n\n\nOnce you've chosen a feature, it's time to write scenarios that describe each part of it. Each scenario follows a very specific pattern. Start by giving it a name.\nThe body of a scenario is made up of three different parts, let's use: \nGiven\n, \nWhen\n and \nThen\n.\n\n\nThe first is Given, which describes the initial state of the system for the scenario. This is the only place where you can describe things that the user can't do. In this case, the \"site administrator\" can't magically put 5 news entries in the database, but that's ok. To have more than one Given statement, start the next line with And.\n\n\nThe second part of each scenario is When, which describes the actual action that this user is taking. Finally, Then is used to describe what our user can see at the end of the scenario.\n\n\nThe exact language you use in your scenarios is up to you - just make sure to follow the Given, When, Then format. Each line in the scenario is called a \"step\", and should plainly describe what the user is doing and seeing.\n\n\nIf we didn't go any further, we would at least have a standard way of describing our features. Writing scenarios also makes you think through each feature in more detail. When you're finished, you've got a blueprint for exactly what you need to develop, written in language that your client can understand.\n\n\nFeature: Remote fence control API\nIn order to control fence security from anywhere\nAs an API user\nI need to be able to POST JSON instructions that turn fences on/off\n\n\nFeature: Delicious humans\nIn order to be entertained\nAs a dinosaur\nI need to be able to watch delicious humans pass by me all day\n\nFeature: Add a news entry\n    Scenario: Add a new news entry\n    Given I am on \"/admin/news\"\n    When I click \"New entry\"\n    And I fill in \"Title\" with \"Alan Grant does not endorse the park!\"\n    And I press \"Save\"\n    Then I should see \"Your article has been saved\"\n\n\n\nYou can also add test data for your scenario, which can also be referred to as examples:\n\n\nFeature: Add a news entry\n    Scenario Outline: Add a new news entry\n    Given I am on \"\nURL\n\"\n    When I click \"\nbutton\n\"\n    And I fill in \"\nform\n\" with \"\ntext\n\"\n    And I press \"Save\"\n    Then I should see \"\nexpectedResult\n\"\n\n    Examples:\n    | URL | button | form | text | expectedResult |\n    | /admin/news | New entry | Title | Alan Grant does not endorse the park! | Your article has been saved |\n\n\n\nExecution \n Results\n\n\nDepending on test coverage, manual, unit and automation reports are handled separately. An example, below work in progress, would use the @TAGS to identify a test type, a feature \n scenario.\n\n\n\n\n\n\n\n\nTags\n\n\nP/F\n\n\nDescription\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n@type1 @epic1 @scenario1\n\n\nP\n\n\nSome abstract on the feature.\n\n\nTested on iOS and Android in addition to IE and Chrome on Windows.\n\n\n\n\n\n\n@type2 @epic2 @scenario2\n\n\nF\n\n\nSome abstract on the feature.\n\n\nSee more in TICKET-XXX", 
            "title": "Methodology"
        }, 
        {
            "location": "/methodology/#qa-test-planning-methodology", 
            "text": "", 
            "title": "QA Test Planning Methodology"
        }, 
        {
            "location": "/methodology/#planning", 
            "text": "To begin the planning stage, QA's have to meet with developers and/or product owners and/or clients to discuss the documentation and project scope prior to implementation to begin the planning stage. Documentation comes in various forms, from paper to electronic, but no matter what kind, these product/project documents have to be analyzed. The time reading it should also be time dedicated to looking for any discrepancies and issues, meanwhile also building and spec'ing test dependancies, frameworks and cases. The product document is examined thoroughly for complexity, UI features, functionality, issues and most importantly  requirements ; some aspects may need to be changed to work. Analysis as a QA is feedback for the developers.", 
            "title": "Planning"
        }, 
        {
            "location": "/methodology/#implementation", 
            "text": "Feedback to developers signals the beginning of the implementation stage. The product is placed into the warm and loving hands of development. During this stage QA's begin to build test cases for the upcoming product. These cases are built with the concepts in Test Plans parenting Test Cases/Scenarios. Each test is designed to examine the product as per the Scope Document(s). After further documentation is processed, the QA's begin building test cases again to examine the code that has been provided. The testing goes on until all elements of the product have been tested thoroughly.  Software testing is the process of validating and verifying that a software program/application/product:   Meets the requirements that guided its design and development;  Works as expected; and  Can be implemented with the same characteristics in other environments", 
            "title": "Implementation"
        }, 
        {
            "location": "/methodology/#delivery-maintenance", 
            "text": "This is the bulk of the QA work. Regression tests are executed during this phase, Manual or Automation. After more tests have been built to meet the needs of client based functionalities, a UAT has to be drafted and handed out to the client(s), internal and/or external.  This UAT (User Acceptance Test) tests all up-to-date functions of the software. During this phase it is also key to build test plans, tests that check the code in a regimented fashion. These tests also have to be executed, heeding to the Test Schedule. All test results have to be delivered, in a Test Execution Report.  Everything has to be reproducible.", 
            "title": "Delivery &amp; Maintenance"
        }, 
        {
            "location": "/methodology/#test-types", 
            "text": "During the planning stage, QA Testers build test cases. Test cases and test plans require a full assessment of the documentation.  There are many kinds of testing and they include but are not limited to:     Type  Description      Black-Box  Internal system design is not considered in this type of testing. Tests are based on requirements and functionality.    White -Box  This testing is based on knowledge of the internal logic of an application's code. Also known as Glass box Testing. Internal software and code working should be known for this type of testing. Tests are based on coverage of code statements, branches, paths, conditions.    Unit  Testing of individual software components. Typically done by the programmer and not by testers, as it requires detailed knowledge of the internal program design and code. May require developing test driver modules or test harnesses.    Incremental Integration  Bottom up approach for testing i.e. continuous testing of an application as new functionality is added; Application functionality and modules should be independent enough to test separately. Done by programmers or by testers.    Integration  Testing of integrated modules to verify functionality after integration. Modules are typically code modules, individual applications, client and server applications on a network, etc. This type of testing is especially relevant to client/server and distributed systems.    Functional  This type of testing ignores the internal parts and focus on the output is as per requirement or not. Black-box type testing geared to functional requirements of an application.    System  Entire system is tested as per the requirements. Black-box type testing that is based on overall requirements/specifications. Covers all combined parts of a system.    End-to-End  Similar to system testing, involves testing of a complete application environment in a situation that mimics real-world use, such as interacting with a database, using network communications, or interacting with other hardware, applications, or systems if appropriate.    Sanity  Testing to determine if a new software version is performing well enough to accept it for a major testing effort. If application is crashing for initial use then system is not stable enough for further testing and build or application is assigned to fix.    Regression  Testing the application as a whole for the modification in any module or functionality. Difficult to cover all the system in regression testing so typically automation tools are used for these testing types.    Acceptance, UAT  Normally this type of testing is done to verify if system meets the customer specified requirements. Users or customers use this testing to determine whether to accept application.    Load  It's a performance testing to check system behavior under load. Testing an application under heavy loads, such as testing of a web site under a range of loads to determine at what point the system response time degrades or fails.    Stress  System is stressed beyond its specifications to check how and when it fails. Performed under heavy load like putting large number beyond storage capacity, complex database queries, continuous input to system or database load.    Performance  Term often used interchangeably with stress and load testing. To check whether system meets performance requirements. Used different performance and load tools to do this.    Usability  User-friendliness check. Application flow is tested, Can new user understand the application easily, Proper help documented whenever user stuck at any point. Basically system navigation is checked in this testing.    Installation  Tested for full, partial, or upgrade install/uninstall processes on different operating systems under different hardware, software environment.    Recovery  Testing how well a system recovers from crashes, hardware failures, or other catastrophic problems.    Security  Can system be penetrated by any hacking way. Testing how well the system protects against unauthorized internal or external access. Checked if system, database is safe from external attacks.    Compatibility  Testing how well software performs in a particular hardware/software/operating system/network environment and different combinations of above.    Comparison  Comparison of product strengths and weaknesses with previous versions or other similar products.    Alpha  In house virtual user environment can be created for this type of testing. Testing is done at the end of development. Still minor design changes may be made as a result of such testing.    Beta  Testing typically done by end-users or others. Final testing before releasing application for commercial purpose.", 
            "title": "Test Types"
        }, 
        {
            "location": "/methodology/#test-plans", 
            "text": "Early in the deployment planning phase, the testing team creates a test plan. The test plan defines the objectives and scope of the testing effort, and identifies the methodology that your team will use to conduct tests. It also identifies the hardware, software, and tools required for testing and the features and functions that will be tested. A well-rounded test plan notes any risk factors that jeopardize testing and includes a testing schedule.  If your testing team is divided into technology sub-teams, each sub-team should develop a test plan for that team's specific technology area. For example, the networking team would write a test plan for testing networking features. All members of each sub-team should review and approve its team's test plan before it is integrated into the general test plan.", 
            "title": "Test Plans"
        }, 
        {
            "location": "/methodology/#defining-test-scope", 
            "text": "In the scope and objectives section of the test plan, the testing team describes specifically what you want your testing to accomplish.  Also, you need to define the scope of your testing by identifying what you will test and what you will not. For example, you might limit your testing of client computer hardware to the minimum supported configurations or to the standard configurations.", 
            "title": "Defining Test Scope"
        }, 
        {
            "location": "/methodology/#identify-requirements", 
            "text": "Requirement  Description      Hardware  List the hardware that you need to conduct tests. Include components such as video cards, modems, routers and external drives.    Software  List the software that you need to test the compatibility of your applications within defined environments.    Environments  Include databases or environments that you need to prepare for testing applications. Also include a description of the resources that you need to populate the databases, such as personal and business data.    Personnel  Identify the number of testers you need and the skill level required. Include consultants and other support personnel, as necessary.    Training  For example, any additional learning or training that your testers need to complete prior to testing their assigned applications or technologies.    Tools  Include all tools or scripts that you need to automate testing and to track results.", 
            "title": "Identify Requirements"
        }, 
        {
            "location": "/methodology/#list-features", 
            "text": "List all the features or aspects of features that need to be tested. This part of the test plan describes what to test, not how to test. For example:   Contact Form  Checkout/Shopping Cart  Registration   Log In  Account Management  RMA Return Management", 
            "title": "List Features"
        }, 
        {
            "location": "/methodology/#define-acceptance-criteria", 
            "text": "Acceptance criteria is to be determined by the User Stories that make up the MVP. The user stories will follow but the test coverage has to ensure:   The functionality of each feature and service that implemented  Interoperability with existing components and systems in the production environment, both during the phase-in period and after the environment has been rolled out  Hardware and driver compatibility for every type of client computer  Application compatibility for every application  Application compatibility for every server application  Optimization of configurations, such as those for standardized desktops on client computers  Baselines (a range of measurements derived from performance monitoring that represents acceptable performance under typical conditions) for performance monitoring  Baselines and stress tests for capacity planning  Procedures for deployment and post-deployment administration, such as procedures for upgrading a client computer and for backing out of a faulty rollout process.  Uses the required tools and utilities  Describes the risk factors that could prevent the successful completion of all required tests.", 
            "title": "Define Acceptance Criteria"
        }, 
        {
            "location": "/methodology/#create-a-test-schedule", 
            "text": "Draft a preliminary schedule that includes each test listed in the test plan. The schedule can help you coordinate test lab use among sub-teams. Assign a team member, ideally the test lab manager, if your team has one, to maintain and update the lab schedule. Having an up-to-date schedule is critical when unscheduled lab requests are submitted.  Testing occurs through all phases of development. The dates below denote stress, forced-error, UAT, End to End and System testing prior to client review. Testing will continue to occur during development and integration. Automation and unit tests to be completed in parallel to manual testing, if feasible and necessary. For example:     Task Name  Duration (days)  Start  Finish      Application Name  36  MM/DD/YYYY  MM/DD/YYYY    Development  22  MM/DD/YYYY  MM/DD/YYYY    QA Review #1 - Theta Testing  2  MM/DD/YYYY  MM/DD/YYYY    Development (Revisions)  2  MM/DD/YYYY  MM/DD/YYYY    Deployment to QA Environment  2  MM/DD/YYYY  MM/DD/YYYY    QA Review #2 - Beta Testing  1  MM/DD/YYYY  MM/DD/YYYY    Deployment to Stage or Alpha Environment  2  MM/DD/YYYY  MM/DD/YYYY    Final QA - Alpha Testing  5  MM/DD/YYYY  MM/DD/YYYY", 
            "title": "Create a Test Schedule"
        }, 
        {
            "location": "/methodology/#define-epic-user-stories", 
            "text": "List all the features or aspects of features that need to be tested. This part of the test plan describes what to test, not how to test. The terminology and format are not limited to any type. However, user stories are business-readable and proficient.", 
            "title": "Define Epic User Stories"
        }, 
        {
            "location": "/methodology/#user-stories", 
            "text": "", 
            "title": "User Stories *"
        }, 
        {
            "location": "/methodology/#traditional-user-story-template", 
            "text": "As a  role ,\nI want  goal/desire ,\nSo that  benefit   The \"so that\" clause as optional.", 
            "title": "Traditional user-story template"
        }, 
        {
            "location": "/methodology/#hunting-the-value", 
            "text": "In order to  receive benefit ,\nAs a  role ,\nI want  goal/desire", 
            "title": "Hunting the value"
        }, 
        {
            "location": "/methodology/#five-ws-specifies", 
            "text": "As  who   when   where ,\nI  what ,\nBecause  why", 
            "title": "Five W's specifies:"
        }, 
        {
            "location": "/methodology/#define-scenarios", 
            "text": "A test case or scenario is a detailed procedure that fully tests a feature or an aspect of a feature. Whereas the test plan describes what is to be tested, a test case describes how to perform a particular test. You need to develop a test case for each test listed in the test plan or test specification.  Test cases should be written by someone who understands the function or technology being tested and should go through a peer review.  Test cases include information such as the following:   Purpose of the test  Special hardware requirements, such as a modem  Special software requirements, such as a tool  Specific setup or configuration requirements  Description of how to perform the test  Expected results or success criteria for the test  Designing test cases can be a time-consuming phase in your testing schedule. Although you might be tempted to take shortcuts, the time you spend will pay off in the long run.   You can conduct tests faster when they are carefully planned. Otherwise, testers spend time debugging and rerunning tests.  Organizations take a variety of approaches to documenting test cases; these range from developing detailed, recipe-like steps to writing general descriptions. In detailed test cases, the steps describe exactly how to perform the test. In descriptive test cases, the tester decides at the time of the test how to perform the test and what data to use.  Some advantages of detailed test cases are that they are reproducible and they are easier to automate. This approach is particularly important if you plan to compare the results of tests over time, such as when you are optimizing configurations. A disadvantage of detailed test cases is that they are more time-consuming to develop and maintain. On the other hand, test cases that are open to interpretation are not repeatable and can lead to debugging time that pertains more to the test itself than to what is being tested.  It is recommended that you find a compromise between the two extremes, one that tends toward more detail. Balance thoroughness with practicality to reach your goal of test integrity and manageability.", 
            "title": "Define Scenarios"
        }, 
        {
            "location": "/methodology/#advantages-of-gherkin-based-tests", 
            "text": "Scenarios can be written in a multitude of ways. However, when it comes to standardization, its hard to hit the nail on the head as uniquely as Gherkin does. Even if AUT uses a JavaScript or fully-Manual testing solution, business-readable scenarios go along way for", 
            "title": "Advantages of Gherkin-based Tests"
        }, 
        {
            "location": "/methodology/#gherkin-scenarios", 
            "text": "Once you've chosen a feature, it's time to write scenarios that describe each part of it. Each scenario follows a very specific pattern. Start by giving it a name.\nThe body of a scenario is made up of three different parts, let's use:  Given ,  When  and  Then .  The first is Given, which describes the initial state of the system for the scenario. This is the only place where you can describe things that the user can't do. In this case, the \"site administrator\" can't magically put 5 news entries in the database, but that's ok. To have more than one Given statement, start the next line with And.  The second part of each scenario is When, which describes the actual action that this user is taking. Finally, Then is used to describe what our user can see at the end of the scenario.  The exact language you use in your scenarios is up to you - just make sure to follow the Given, When, Then format. Each line in the scenario is called a \"step\", and should plainly describe what the user is doing and seeing.  If we didn't go any further, we would at least have a standard way of describing our features. Writing scenarios also makes you think through each feature in more detail. When you're finished, you've got a blueprint for exactly what you need to develop, written in language that your client can understand.  Feature: Remote fence control API\nIn order to control fence security from anywhere\nAs an API user\nI need to be able to POST JSON instructions that turn fences on/off\n\n\nFeature: Delicious humans\nIn order to be entertained\nAs a dinosaur\nI need to be able to watch delicious humans pass by me all day\n\nFeature: Add a news entry\n    Scenario: Add a new news entry\n    Given I am on \"/admin/news\"\n    When I click \"New entry\"\n    And I fill in \"Title\" with \"Alan Grant does not endorse the park!\"\n    And I press \"Save\"\n    Then I should see \"Your article has been saved\"  You can also add test data for your scenario, which can also be referred to as examples:  Feature: Add a news entry\n    Scenario Outline: Add a new news entry\n    Given I am on \" URL \"\n    When I click \" button \"\n    And I fill in \" form \" with \" text \"\n    And I press \"Save\"\n    Then I should see \" expectedResult \"\n\n    Examples:\n    | URL | button | form | text | expectedResult |\n    | /admin/news | New entry | Title | Alan Grant does not endorse the park! | Your article has been saved |", 
            "title": "Gherkin scenarios"
        }, 
        {
            "location": "/methodology/#execution-results", 
            "text": "Depending on test coverage, manual, unit and automation reports are handled separately. An example, below work in progress, would use the @TAGS to identify a test type, a feature   scenario.     Tags  P/F  Description  Notes      @type1 @epic1 @scenario1  P  Some abstract on the feature.  Tested on iOS and Android in addition to IE and Chrome on Windows.    @type2 @epic2 @scenario2  F  Some abstract on the feature.  See more in TICKET-XXX", 
            "title": "Execution &amp; Results"
        }, 
        {
            "location": "/bugs/", 
            "text": "Reproducing Bugs\n\n\nHow to File a Bug\n\n\nWhen filing bugs, or jotting down \nsteps-to-reproduce\n, you should try to have the following information:\n\n\n\n\nWhere the issue was observed?\n \n\n\nWhat site was this on?\n\n\nWhat browser or device were you using?\n Provide a URL, account name, device model, version if possible.\n\n\nWhat are the steps to reproduce the issue?\n Be as detailed as possible. Sometimes issues can seem really obvious to you, and the steps you performed seem unimportant. They aren't. Developers need this information to help debug the issue. QA needs this information to be able to completely validate that the issue was fixed. \n\n\nWhat was the expected result?\n Again, even if it seems obvious, go ahead and put the obvious expected result. This piece of information is not only vital in knowing how to fix the issue, but it's vital to QA to know what success will look like for the issue. In addition to the documentation aspect, this information can also save much wasted time and effort, as it lets all parties know what's expected. Maybe what the reporter expected is actually incorrect, and it is performing as it should. Maybe what the reporter expects is different from what the person working on the issue thinks should happen. Having the expectation allows that conversation to take place before work is started on the ticket and time is wasted.\n\n\nWhat was the actual result? What actually happened when you performed the steps in #2?\n Please be descriptive in what the actual result was. This helps the person working on the ticket ensure that they have accurately reproduced the issue. It also exists to provide a clear contrast for the expected result.\n\n\n\n\nOther things that you should include in your bug ticket:\n\n\n\n\nAny specification documents relating to the functionality. This helps provide a trail and additional information when working on the issue.\n\n\nScreenshots or video of the issue. Sometimes a picture is worth a thousand words, and a video is worth exponentially more than that. These can provide insight into the issue that a written description never could.\n\n\n\n\nBug Tracking Software\n\n\nJIRA\n\n\nThe proprietary issue tracking product developed by Atlassian. It provides bug tracking, issue tracking, and project management functions.", 
            "title": "Bugs"
        }, 
        {
            "location": "/bugs/#reproducing-bugs", 
            "text": "", 
            "title": "Reproducing Bugs"
        }, 
        {
            "location": "/bugs/#how-to-file-a-bug", 
            "text": "When filing bugs, or jotting down  steps-to-reproduce , you should try to have the following information:   Where the issue was observed?    What site was this on?  What browser or device were you using?  Provide a URL, account name, device model, version if possible.  What are the steps to reproduce the issue?  Be as detailed as possible. Sometimes issues can seem really obvious to you, and the steps you performed seem unimportant. They aren't. Developers need this information to help debug the issue. QA needs this information to be able to completely validate that the issue was fixed.   What was the expected result?  Again, even if it seems obvious, go ahead and put the obvious expected result. This piece of information is not only vital in knowing how to fix the issue, but it's vital to QA to know what success will look like for the issue. In addition to the documentation aspect, this information can also save much wasted time and effort, as it lets all parties know what's expected. Maybe what the reporter expected is actually incorrect, and it is performing as it should. Maybe what the reporter expects is different from what the person working on the issue thinks should happen. Having the expectation allows that conversation to take place before work is started on the ticket and time is wasted.  What was the actual result? What actually happened when you performed the steps in #2?  Please be descriptive in what the actual result was. This helps the person working on the ticket ensure that they have accurately reproduced the issue. It also exists to provide a clear contrast for the expected result.   Other things that you should include in your bug ticket:   Any specification documents relating to the functionality. This helps provide a trail and additional information when working on the issue.  Screenshots or video of the issue. Sometimes a picture is worth a thousand words, and a video is worth exponentially more than that. These can provide insight into the issue that a written description never could.", 
            "title": "How to File a Bug"
        }, 
        {
            "location": "/bugs/#bug-tracking-software", 
            "text": "", 
            "title": "Bug Tracking Software"
        }, 
        {
            "location": "/bugs/#jira", 
            "text": "The proprietary issue tracking product developed by Atlassian. It provides bug tracking, issue tracking, and project management functions.", 
            "title": "JIRA"
        }, 
        {
            "location": "/test_plans/reading_pro_uat_plan/", 
            "text": "", 
            "title": "Reading Pro UAT"
        }, 
        {
            "location": "/charles/", 
            "text": "Charles (for Blackbox Testing)\n\n\nIf prefferred, download \nCharles\n. This tool, like JMeter, is powerful tool that tells you what exactly is going on when using a website. It can even be used to sniff iOS and Android http/https requests, let alone API's.  \n\n\nAs soon as you open Charles and play around outside the app, you'll notice data being collected in the left-tab. Every HTTP/HTTPS request being made is monitored. The data obtained will help test and make API/URI/URL requests. \n\n\nAUT\n = Application Under Test\n\n\n\n\n\n\n\n\nStep\n\n\nDescription\n\n\nScreenshot\n\n\n\n\n\n\n\n\n\n\n1\n\n\nOpen the AUT in your browser of choice. For this example, lets use Chrome on Mac OS X.\n\n\n\n\n\n\n\n\n2\n\n\nOpen \nCharles\n.\n\n\n\n\n\n\n\n\n3\n\n\nFrom the toolbar, select \nSettings\n.\n\n\n\n\n\n\n\n\n4\n\n\nFrom Settings, select \nRecorder Settings\n.\n\n\n\n\n\n\n\n\n5\n\n\nFrom Recorder Settings, select \nInclude\n tab.\n\n\n\n\n\n\n\n\n6\n\n\nFrom Include Tab, press \nAdd\n tab.\n\n\n\n\n\n\n\n\n7\n\n\nIn the \"Include\" section, limit the recorder to sniff only requests made from \nScholastic\n servers. This is called whitelisting.\n\n\n\n\n\n\n\n\n\n\nYour recording settings should reflect as follows. Be sure to include the server of the click-through. Refresh the AUT.\n\n\nUntil now your requests log should've been either empty or full of other requests from your computer. You should see requests being populated from \nScholastic\n servers.", 
            "title": "Charles"
        }, 
        {
            "location": "/charles/#charles-for-blackbox-testing", 
            "text": "If prefferred, download  Charles . This tool, like JMeter, is powerful tool that tells you what exactly is going on when using a website. It can even be used to sniff iOS and Android http/https requests, let alone API's.    As soon as you open Charles and play around outside the app, you'll notice data being collected in the left-tab. Every HTTP/HTTPS request being made is monitored. The data obtained will help test and make API/URI/URL requests.   AUT  = Application Under Test     Step  Description  Screenshot      1  Open the AUT in your browser of choice. For this example, lets use Chrome on Mac OS X.     2  Open  Charles .     3  From the toolbar, select  Settings .     4  From Settings, select  Recorder Settings .     5  From Recorder Settings, select  Include  tab.     6  From Include Tab, press  Add  tab.     7  In the \"Include\" section, limit the recorder to sniff only requests made from  Scholastic  servers. This is called whitelisting.      Your recording settings should reflect as follows. Be sure to include the server of the click-through. Refresh the AUT.  Until now your requests log should've been either empty or full of other requests from your computer. You should see requests being populated from  Scholastic  servers.", 
            "title": "Charles (for Blackbox Testing)"
        }, 
        {
            "location": "/about/", 
            "text": "About\n\n\nThis project uses \nmkdocs\n, for full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n\n\n\n\nAuthors\n\n\nSajjad Hossain\n\n\n\n\nGitHub\n  \n\n\nTwitter", 
            "title": "About"
        }, 
        {
            "location": "/about/#about", 
            "text": "This project uses  mkdocs , for full documentation visit  mkdocs.org .", 
            "title": "About"
        }, 
        {
            "location": "/about/#commands", 
            "text": "mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/about/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/about/#authors", 
            "text": "", 
            "title": "Authors"
        }, 
        {
            "location": "/about/#sajjad-hossain", 
            "text": "GitHub     Twitter", 
            "title": "Sajjad Hossain"
        }
    ]
}